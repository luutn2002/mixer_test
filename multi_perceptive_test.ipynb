{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d7817fa-6886-44a6-9c5a-404c5f82cb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "No checkpoint is detected in experiment_checkpoint folder, training from begining\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Loss: 6.913926  [    0/1281167]\n",
      "Loss: 6.982344  [ 2000/1281167]\n",
      "Loss: 6.950527  [ 4000/1281167]\n",
      "Loss: 6.943144  [ 6000/1281167]\n",
      "Loss: 6.927519  [ 8000/1281167]\n",
      "Loss: 6.978594  [10000/1281167]\n",
      "Loss: 6.925078  [12000/1281167]\n",
      "Loss: 6.986582  [14000/1281167]\n",
      "Loss: 6.964375  [16000/1281167]\n",
      "Loss: 6.936543  [18000/1281167]\n",
      "Loss: 6.933496  [20000/1281167]\n",
      "Loss: 6.968066  [22000/1281167]\n",
      "Loss: 6.946777  [24000/1281167]\n",
      "Loss: 6.950899  [26000/1281167]\n",
      "Loss: 6.957930  [28000/1281167]\n",
      "Loss: 6.970723  [30000/1281167]\n",
      "Loss: 6.974141  [32000/1281167]\n",
      "Loss: 6.940137  [34000/1281167]\n",
      "Loss: 6.910781  [36000/1281167]\n",
      "Loss: 6.933066  [38000/1281167]\n",
      "Loss: 6.945762  [40000/1281167]\n",
      "Loss: 6.937207  [42000/1281167]\n",
      "Loss: 6.916739  [44000/1281167]\n",
      "Loss: 6.948301  [46000/1281167]\n",
      "Loss: 6.957402  [48000/1281167]\n",
      "Loss: 6.950137  [50000/1281167]\n",
      "Loss: 6.944473  [52000/1281167]\n",
      "Loss: 6.910215  [54000/1281167]\n",
      "Loss: 6.920586  [56000/1281167]\n",
      "Loss: 6.913340  [58000/1281167]\n",
      "Loss: 6.924551  [60000/1281167]\n",
      "Loss: 6.933809  [62000/1281167]\n",
      "Loss: 6.920527  [64000/1281167]\n",
      "Loss: 6.936055  [66000/1281167]\n",
      "Loss: 6.926367  [68000/1281167]\n",
      "Loss: 6.958574  [70000/1281167]\n",
      "Loss: 6.944101  [72000/1281167]\n",
      "Loss: 6.923203  [74000/1281167]\n",
      "Loss: 6.907246  [76000/1281167]\n",
      "Loss: 6.942266  [78000/1281167]\n",
      "Loss: 6.904570  [80000/1281167]\n",
      "Loss: 6.903359  [82000/1281167]\n",
      "Loss: 6.967461  [84000/1281167]\n",
      "Loss: 6.867988  [86000/1281167]\n",
      "Loss: 6.947989  [88000/1281167]\n",
      "Loss: 6.959610  [90000/1281167]\n",
      "Loss: 6.917598  [92000/1281167]\n",
      "Loss: 7.020761  [94000/1281167]\n",
      "Loss: 7.084180  [96000/1281167]\n",
      "Loss: 7.001660  [98000/1281167]\n",
      "Loss: 7.088115  [100000/1281167]\n",
      "Loss: 6.975791  [102000/1281167]\n",
      "Loss: 6.914912  [104000/1281167]\n",
      "Loss: 6.890527  [106000/1281167]\n",
      "Loss: 6.961016  [108000/1281167]\n",
      "Loss: 6.920508  [110000/1281167]\n",
      "Loss: 6.974570  [112000/1281167]\n",
      "Loss: 6.785498  [114000/1281167]\n",
      "Loss: 6.779238  [116000/1281167]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W python_anomaly_mode.cpp:104] Warning: Error detected in LogSoftmaxBackward. Traceback of forward call that caused the error:\n",
      "  File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/ccl/testing/lib/python3.8/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/ccl/testing/lib/python3.8/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/ccl/testing/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 677, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/ccl/testing/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.8/asyncio/events.py\", line 81, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/ccl/testing/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 457, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/ccl/testing/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 446, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/ccl/testing/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 353, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/ccl/testing/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 648, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/ccl/testing/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 353, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/ccl/testing/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/ccl/testing/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2898, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/ccl/testing/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2944, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/home/ccl/testing/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/ccl/testing/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3169, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/ccl/testing/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3361, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/home/ccl/testing/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3441, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_15014/3658479269.py\", line 106, in <module>\n",
      "    train(train_dataloader, model, loss_fn, optimizer, scheduler, scaler)\n",
      "  File \"/tmp/ipykernel_15014/3658479269.py\", line 55, in train\n",
      "    loss = loss_fn(pred, y)\n",
      "  File \"/home/ccl/testing/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/ccl/testing/lib/python3.8/site-packages/torch/nn/modules/loss.py\", line 1120, in forward\n",
      "    return F.cross_entropy(input, target, weight=self.weight,\n",
      "  File \"/home/ccl/testing/lib/python3.8/site-packages/torch/nn/functional.py\", line 2824, in cross_entropy\n",
      "    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)\n",
      " (function _print_stack)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function 'LogSoftmaxBackward' returned nan values in its 0th output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15014/3658479269.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTOTAL_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {t+1}\\n-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mcurrent_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriting_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCSV_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcurrent_acc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0minit_acc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_15014/3658479269.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer, scheduler, scaler)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/testing/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/testing/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Function 'LogSoftmaxBackward' returned nan values in its 0th output."
     ]
    }
   ],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.autograd import set_detect_anomaly\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch import device, save, load, no_grad, float\n",
    "from torch.cuda import is_available\n",
    "from numpy import NINF\n",
    "from pandas import DataFrame\n",
    "import time\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from models import MultiPerpectiveMixer\n",
    "from configs import get_MultiPerceptiveMixer_config_224, get_imagenet_config\n",
    "from input_piplines import get_imagenet_loader\n",
    "\n",
    "TOTAL_EPOCHS = 200\n",
    "CSV_DIR = './training_record_224.csv'\n",
    "CHECKPOINT_DIR = './experiment_checkpoint'\n",
    "CHECKPOINT_FILE = CHECKPOINT_DIR + '/highest_accuracy_ckpt'\n",
    "\n",
    "#device = device(\"cpu\")\n",
    "device = device(\"cuda\" if is_available() else \"cpu\")\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "train_dataloader, test_dataloader = get_imagenet_loader(get_imagenet_config())\n",
    "\n",
    "model = MultiPerpectiveMixer(get_MultiPerceptiveMixer_config_224()).cuda()\n",
    "#model.to(device)\n",
    "set_detect_anomaly(True)\n",
    "\n",
    "loss_fn = CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-9)\n",
    "lmbda = lambda epoch: 1.03 ** epoch\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lmbda)\n",
    "scaler = GradScaler()\n",
    "\n",
    "if not os.path.isfile(CSV_DIR):\n",
    "    data = DataFrame(columns=['Epoch', 'Validation Loss', 'Validation Accuracy'])\n",
    "    data.to_csv(CSV_DIR, index=False)\n",
    "    \n",
    "def train(dataloader, model, loss_fn, optimizer, scheduler, scaler):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.cuda(), y.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast():\n",
    "        # Compute prediction error\n",
    "            pred = model(X)\n",
    "            #y = y.squeeze(1).long()\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scheduler.step()\n",
    "   \n",
    "        scaler.update()\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"Loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn, current_epoch, writing_dir=None, sleep_and_clear=True):\n",
    "    \n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    with no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.cuda(), y.cuda()\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(float).sum().item()\n",
    "            \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    correct = correct*100\n",
    "    print(f\"Test Error: \\n Pixel-wise accuracy: {correct}%, Average loss: {test_loss} \\n\")\n",
    "    \n",
    "    if writing_dir is not None:    \n",
    "        current_data = DataFrame([[current_epoch, correct, test_loss]])\n",
    "        current_data.to_csv(writing_dir, index=False, mode='a', header=False)\n",
    "    \n",
    "    if sleep_and_clear:   \n",
    "        time.sleep(5)\n",
    "        clear_output()\n",
    "    \n",
    "    return correct\n",
    "            \n",
    "\n",
    "\n",
    "#Train new model\n",
    "if not os.listdir(CHECKPOINT_DIR):\n",
    "    print(\"No checkpoint is detected in experiment_checkpoint folder, training from begining\")\n",
    "    init_acc = NINF\n",
    "    \n",
    "    for t in range(TOTAL_EPOCHS):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train(train_dataloader, model, loss_fn, optimizer, scheduler, scaler)\n",
    "        current_acc = test(test_dataloader, model, loss_fn, t+1, writing_dir=CSV_DIR)\n",
    "        if current_acc > init_acc:\n",
    "            init_acc = current_acc\n",
    "            save({\n",
    "            'epoch': t+1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'testing_accuracy': current_acc\n",
    "            }, CHECKPOINT_FILE)\n",
    "    \n",
    "    save(model, f\"./full_model/epoch_{total_epochs}\")\n",
    "            \n",
    "#Training from checkpoint\n",
    "else:\n",
    "    checkpoint = load(CHECKPOINT_FILE)\n",
    "    print(f\"Checkpoint loaded, last epoch is: {checkpoint['epoch']}, testing accuracy: {checkpoint['testing_accuracy']}%\")\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    init_acc = NINF\n",
    "    \n",
    "    for t in range(checkpoint['epoch'], TOTAL_EPOCHS):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train(train_dataloader, model, loss_fn, optimizer, scheduler, scaler)\n",
    "        current_acc = test(test_dataloader, model, loss_fn, t+1, writing_dir=CSV_DIR)\n",
    "        if current_acc > init_acc:\n",
    "            init_acc = current_acc\n",
    "            save({\n",
    "            'epoch': t+1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'testing_accuracy': current_acc\n",
    "            }, CHECKPOINT_FILE)\n",
    "    \n",
    "    save(model, f\"./full_model/epoch_{total_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6771ccf7-1812-4113-ac38-5a2b4a353400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.6141e+00, -1.4625e+00, -1.4219e+00, -1.3742e+00, -1.3620e+00,\n",
      "        -1.3421e+00, -1.2446e+00, -1.2345e+00, -1.2104e+00, -1.1962e+00,\n",
      "        -1.1849e+00, -1.1770e+00, -1.1744e+00, -1.1742e+00, -1.1655e+00,\n",
      "        -1.1442e+00, -1.1314e+00, -1.1074e+00, -1.1010e+00, -1.0823e+00,\n",
      "        -1.0718e+00, -1.0702e+00, -1.0687e+00, -1.0638e+00, -1.0608e+00,\n",
      "        -1.0314e+00, -1.0301e+00, -1.0032e+00, -9.9681e-01, -9.9649e-01,\n",
      "        -9.9272e-01, -9.8591e-01, -9.8571e-01, -9.7906e-01, -9.6867e-01,\n",
      "        -9.5742e-01, -9.5215e-01, -9.4935e-01, -9.4401e-01, -9.4349e-01,\n",
      "        -9.3328e-01, -9.2968e-01, -9.2898e-01, -9.2796e-01, -9.2351e-01,\n",
      "        -9.2131e-01, -9.2039e-01, -9.1767e-01, -9.1317e-01, -9.0841e-01,\n",
      "        -9.0334e-01, -8.9906e-01, -8.9843e-01, -8.8710e-01, -8.7757e-01,\n",
      "        -8.7524e-01, -8.7282e-01, -8.7235e-01, -8.7090e-01, -8.6513e-01,\n",
      "        -8.6499e-01, -8.5844e-01, -8.5405e-01, -8.4897e-01, -8.4709e-01,\n",
      "        -8.4155e-01, -8.3604e-01, -8.2664e-01, -8.2520e-01, -8.2074e-01,\n",
      "        -8.1864e-01, -8.1801e-01, -8.1057e-01, -8.0783e-01, -8.0678e-01,\n",
      "        -7.9700e-01, -7.9536e-01, -7.9306e-01, -7.9101e-01, -7.9052e-01,\n",
      "        -7.9032e-01, -7.8513e-01, -7.8268e-01, -7.7805e-01, -7.7789e-01,\n",
      "        -7.7045e-01, -7.6915e-01, -7.6753e-01, -7.6105e-01, -7.6082e-01,\n",
      "        -7.5925e-01, -7.5881e-01, -7.5851e-01, -7.4949e-01, -7.4846e-01,\n",
      "        -7.4749e-01, -7.3181e-01, -7.2915e-01, -7.2846e-01, -7.2735e-01,\n",
      "        -7.2440e-01, -7.2183e-01, -7.1762e-01, -7.1500e-01, -7.1346e-01,\n",
      "        -7.1345e-01, -7.0909e-01, -7.0860e-01, -7.0783e-01, -7.0668e-01,\n",
      "        -7.0468e-01, -7.0331e-01, -6.9856e-01, -6.9630e-01, -6.9012e-01,\n",
      "        -6.8440e-01, -6.8234e-01, -6.8214e-01, -6.8102e-01, -6.7152e-01,\n",
      "        -6.7026e-01, -6.6981e-01, -6.6947e-01, -6.6226e-01, -6.6137e-01,\n",
      "        -6.5822e-01, -6.5038e-01, -6.4869e-01, -6.4801e-01, -6.4693e-01,\n",
      "        -6.4673e-01, -6.4256e-01, -6.4239e-01, -6.4091e-01, -6.3902e-01,\n",
      "        -6.3806e-01, -6.3560e-01, -6.3554e-01, -6.2907e-01, -6.2762e-01,\n",
      "        -6.2258e-01, -6.2145e-01, -6.1787e-01, -6.1765e-01, -6.1576e-01,\n",
      "        -6.1501e-01, -6.1451e-01, -6.1259e-01, -6.1257e-01, -6.1078e-01,\n",
      "        -6.0961e-01, -6.0911e-01, -6.0832e-01, -6.0827e-01, -6.0480e-01,\n",
      "        -6.0333e-01, -6.0323e-01, -6.0205e-01, -6.0009e-01, -5.9654e-01,\n",
      "        -5.9636e-01, -5.9448e-01, -5.8962e-01, -5.8900e-01, -5.8453e-01,\n",
      "        -5.8323e-01, -5.7685e-01, -5.7422e-01, -5.7166e-01, -5.6941e-01,\n",
      "        -5.6928e-01, -5.6700e-01, -5.6640e-01, -5.6594e-01, -5.6581e-01,\n",
      "        -5.6088e-01, -5.6037e-01, -5.5749e-01, -5.5337e-01, -5.5147e-01,\n",
      "        -5.5138e-01, -5.5113e-01, -5.5082e-01, -5.4628e-01, -5.4461e-01,\n",
      "        -5.4431e-01, -5.3274e-01, -5.2744e-01, -5.2445e-01, -5.2426e-01,\n",
      "        -5.2371e-01, -5.2323e-01, -5.2275e-01, -5.2196e-01, -5.2076e-01,\n",
      "        -5.1815e-01, -5.1457e-01, -5.1422e-01, -5.1418e-01, -5.1232e-01,\n",
      "        -5.0599e-01, -5.0582e-01, -5.0284e-01, -5.0256e-01, -5.0029e-01,\n",
      "        -4.9838e-01, -4.9532e-01, -4.9502e-01, -4.9304e-01, -4.9295e-01,\n",
      "        -4.8834e-01, -4.8739e-01, -4.8415e-01, -4.8167e-01, -4.7364e-01,\n",
      "        -4.7300e-01, -4.7079e-01, -4.7050e-01, -4.7026e-01, -4.6838e-01,\n",
      "        -4.6832e-01, -4.6807e-01, -4.5858e-01, -4.5703e-01, -4.5677e-01,\n",
      "        -4.5459e-01, -4.5025e-01, -4.4974e-01, -4.4971e-01, -4.4903e-01,\n",
      "        -4.4852e-01, -4.4716e-01, -4.4673e-01, -4.4572e-01, -4.4403e-01,\n",
      "        -4.4233e-01, -4.4163e-01, -4.4117e-01, -4.3979e-01, -4.3720e-01,\n",
      "        -4.3699e-01, -4.3542e-01, -4.3421e-01, -4.3262e-01, -4.3040e-01,\n",
      "        -4.2834e-01, -4.2568e-01, -4.2352e-01, -4.2049e-01, -4.1694e-01,\n",
      "        -4.1306e-01, -4.0329e-01, -4.0169e-01, -3.9760e-01, -3.9720e-01,\n",
      "        -3.9463e-01, -3.9418e-01, -3.9221e-01, -3.9069e-01, -3.8762e-01,\n",
      "        -3.8739e-01, -3.8732e-01, -3.8580e-01, -3.8338e-01, -3.7945e-01,\n",
      "        -3.7252e-01, -3.7079e-01, -3.6771e-01, -3.6569e-01, -3.6487e-01,\n",
      "        -3.6135e-01, -3.6117e-01, -3.5913e-01, -3.5900e-01, -3.5822e-01,\n",
      "        -3.5676e-01, -3.5401e-01, -3.5401e-01, -3.5345e-01, -3.5095e-01,\n",
      "        -3.5070e-01, -3.5013e-01, -3.4665e-01, -3.4591e-01, -3.4550e-01,\n",
      "        -3.4486e-01, -3.4441e-01, -3.4439e-01, -3.4399e-01, -3.4028e-01,\n",
      "        -3.4015e-01, -3.3900e-01, -3.3324e-01, -3.3152e-01, -3.2783e-01,\n",
      "        -3.2495e-01, -3.2265e-01, -3.1928e-01, -3.1621e-01, -3.1299e-01,\n",
      "        -3.0890e-01, -3.0687e-01, -3.0638e-01, -3.0597e-01, -2.9873e-01,\n",
      "        -2.9868e-01, -2.9666e-01, -2.9497e-01, -2.9401e-01, -2.9273e-01,\n",
      "        -2.9206e-01, -2.8980e-01, -2.8978e-01, -2.8849e-01, -2.8818e-01,\n",
      "        -2.8717e-01, -2.8683e-01, -2.8573e-01, -2.8553e-01, -2.8482e-01,\n",
      "        -2.8434e-01, -2.8306e-01, -2.8032e-01, -2.7468e-01, -2.7151e-01,\n",
      "        -2.6764e-01, -2.6596e-01, -2.6519e-01, -2.6348e-01, -2.6342e-01,\n",
      "        -2.6305e-01, -2.6245e-01, -2.5665e-01, -2.5357e-01, -2.5005e-01,\n",
      "        -2.4900e-01, -2.4892e-01, -2.4825e-01, -2.4703e-01, -2.4685e-01,\n",
      "        -2.4559e-01, -2.4279e-01, -2.4205e-01, -2.4122e-01, -2.3800e-01,\n",
      "        -2.3762e-01, -2.3759e-01, -2.3591e-01, -2.2922e-01, -2.2826e-01,\n",
      "        -2.2811e-01, -2.2417e-01, -2.2183e-01, -2.2004e-01, -2.1868e-01,\n",
      "        -2.1756e-01, -2.1611e-01, -2.1587e-01, -2.1442e-01, -2.1338e-01,\n",
      "        -2.1044e-01, -2.0935e-01, -2.0886e-01, -2.0416e-01, -2.0396e-01,\n",
      "        -2.0188e-01, -1.9852e-01, -1.9693e-01, -1.9619e-01, -1.9493e-01,\n",
      "        -1.9124e-01, -1.8463e-01, -1.8341e-01, -1.7893e-01, -1.7862e-01,\n",
      "        -1.7853e-01, -1.7793e-01, -1.7634e-01, -1.7546e-01, -1.7418e-01,\n",
      "        -1.7417e-01, -1.7340e-01, -1.7283e-01, -1.7212e-01, -1.7206e-01,\n",
      "        -1.7015e-01, -1.6870e-01, -1.6830e-01, -1.6803e-01, -1.6566e-01,\n",
      "        -1.6286e-01, -1.6141e-01, -1.6010e-01, -1.5931e-01, -1.5850e-01,\n",
      "        -1.5498e-01, -1.5446e-01, -1.5438e-01, -1.5092e-01, -1.4973e-01,\n",
      "        -1.4861e-01, -1.4796e-01, -1.4357e-01, -1.3979e-01, -1.3934e-01,\n",
      "        -1.3763e-01, -1.3755e-01, -1.3741e-01, -1.3596e-01, -1.3532e-01,\n",
      "        -1.3489e-01, -1.3328e-01, -1.3238e-01, -1.3112e-01, -1.2907e-01,\n",
      "        -1.2581e-01, -1.2148e-01, -1.2140e-01, -1.1909e-01, -1.1825e-01,\n",
      "        -1.1825e-01, -1.1788e-01, -1.1763e-01, -1.1611e-01, -1.1475e-01,\n",
      "        -1.1408e-01, -1.1160e-01, -1.0810e-01, -1.0807e-01, -1.0769e-01,\n",
      "        -1.0646e-01, -1.0441e-01, -1.0435e-01, -1.0141e-01, -9.9894e-02,\n",
      "        -9.9478e-02, -9.9119e-02, -9.6864e-02, -9.5959e-02, -9.5519e-02,\n",
      "        -9.3435e-02, -9.3330e-02, -9.0697e-02, -8.9204e-02, -8.7355e-02,\n",
      "        -8.7247e-02, -8.6877e-02, -8.5776e-02, -8.5155e-02, -8.4695e-02,\n",
      "        -7.9063e-02, -7.8668e-02, -7.3566e-02, -7.2073e-02, -6.9745e-02,\n",
      "        -6.7805e-02, -6.7728e-02, -6.6751e-02, -6.4509e-02, -6.2975e-02,\n",
      "        -6.2775e-02, -6.1549e-02, -6.0683e-02, -6.0122e-02, -5.9975e-02,\n",
      "        -5.9238e-02, -5.7645e-02, -5.5090e-02, -5.3250e-02, -5.1645e-02,\n",
      "        -5.1268e-02, -5.0551e-02, -4.9729e-02, -4.7685e-02, -4.6413e-02,\n",
      "        -4.4379e-02, -4.3605e-02, -4.2754e-02, -4.1491e-02, -4.0084e-02,\n",
      "        -3.7098e-02, -3.5248e-02, -3.3921e-02, -3.3378e-02, -3.1096e-02,\n",
      "        -3.0802e-02, -2.9617e-02, -2.9558e-02, -2.9312e-02, -2.9288e-02,\n",
      "        -2.8598e-02, -2.5546e-02, -2.4706e-02, -2.4108e-02, -2.3402e-02,\n",
      "        -2.3106e-02, -2.0090e-02, -2.0056e-02, -1.9995e-02, -1.8439e-02,\n",
      "        -1.7935e-02, -1.7374e-02, -1.5479e-02, -1.2330e-02, -9.9306e-03,\n",
      "        -7.7344e-03, -7.4854e-03, -4.3327e-03, -2.4625e-03, -1.8618e-03,\n",
      "        -1.2980e-03, -4.3717e-04,  2.6108e-04,  4.8754e-04,  2.9519e-03,\n",
      "         3.7879e-03,  4.4080e-03,  5.4134e-03,  7.3503e-03,  8.5630e-03,\n",
      "         8.5787e-03,  9.3534e-03,  1.1416e-02,  1.2590e-02,  1.3616e-02,\n",
      "         1.4589e-02,  1.4637e-02,  1.5268e-02,  1.7415e-02,  1.7828e-02,\n",
      "         1.8730e-02,  1.9511e-02,  2.3039e-02,  2.7174e-02,  2.7326e-02,\n",
      "         3.1072e-02,  3.1116e-02,  3.3475e-02,  3.5959e-02,  3.7541e-02,\n",
      "         4.4481e-02,  4.4825e-02,  4.5554e-02,  4.7905e-02,  5.0363e-02,\n",
      "         5.1646e-02,  5.1954e-02,  5.5963e-02,  5.6537e-02,  5.9931e-02,\n",
      "         6.0890e-02,  6.1813e-02,  6.5938e-02,  6.7135e-02,  6.7420e-02,\n",
      "         6.7876e-02,  7.3048e-02,  7.4508e-02,  7.5507e-02,  7.7217e-02,\n",
      "         8.0835e-02,  8.0903e-02,  8.0934e-02,  8.0997e-02,  8.1095e-02,\n",
      "         8.4749e-02,  8.5016e-02,  8.6375e-02,  8.7128e-02,  8.8968e-02,\n",
      "         9.0052e-02,  9.3331e-02,  9.4239e-02,  9.5147e-02,  9.7254e-02,\n",
      "         9.8215e-02,  9.9244e-02,  9.9818e-02,  1.0242e-01,  1.0438e-01,\n",
      "         1.0525e-01,  1.0550e-01,  1.0631e-01,  1.0913e-01,  1.1133e-01,\n",
      "         1.1175e-01,  1.1431e-01,  1.1530e-01,  1.1576e-01,  1.1713e-01,\n",
      "         1.1777e-01,  1.1799e-01,  1.2185e-01,  1.2286e-01,  1.2781e-01,\n",
      "         1.3057e-01,  1.3265e-01,  1.3315e-01,  1.3431e-01,  1.3438e-01,\n",
      "         1.4180e-01,  1.4246e-01,  1.4293e-01,  1.4881e-01,  1.4901e-01,\n",
      "         1.4974e-01,  1.5275e-01,  1.5580e-01,  1.5613e-01,  1.6046e-01,\n",
      "         1.6188e-01,  1.6224e-01,  1.6815e-01,  1.7062e-01,  1.7096e-01,\n",
      "         1.7099e-01,  1.7156e-01,  1.7246e-01,  1.7322e-01,  1.7328e-01,\n",
      "         1.7380e-01,  1.7452e-01,  1.7680e-01,  1.8002e-01,  1.8061e-01,\n",
      "         1.8309e-01,  1.8412e-01,  1.8413e-01,  1.8553e-01,  1.8561e-01,\n",
      "         1.8628e-01,  1.8658e-01,  1.8681e-01,  1.8794e-01,  1.9152e-01,\n",
      "         1.9294e-01,  1.9313e-01,  1.9320e-01,  1.9533e-01,  1.9597e-01,\n",
      "         1.9741e-01,  2.0374e-01,  2.0395e-01,  2.0570e-01,  2.0646e-01,\n",
      "         2.1345e-01,  2.1391e-01,  2.1516e-01,  2.1533e-01,  2.1552e-01,\n",
      "         2.1829e-01,  2.1888e-01,  2.2143e-01,  2.2230e-01,  2.2370e-01,\n",
      "         2.2591e-01,  2.2694e-01,  2.2859e-01,  2.2915e-01,  2.3078e-01,\n",
      "         2.3176e-01,  2.3342e-01,  2.3493e-01,  2.3658e-01,  2.3698e-01,\n",
      "         2.4043e-01,  2.4153e-01,  2.4270e-01,  2.4441e-01,  2.4515e-01,\n",
      "         2.4559e-01,  2.5380e-01,  2.5468e-01,  2.5555e-01,  2.5629e-01,\n",
      "         2.5793e-01,  2.5956e-01,  2.5995e-01,  2.6027e-01,  2.6286e-01,\n",
      "         2.6354e-01,  2.6383e-01,  2.6519e-01,  2.6731e-01,  2.6900e-01,\n",
      "         2.6963e-01,  2.7123e-01,  2.7191e-01,  2.7326e-01,  2.7697e-01,\n",
      "         2.7881e-01,  2.7957e-01,  2.7985e-01,  2.8060e-01,  2.8153e-01,\n",
      "         2.8375e-01,  2.8390e-01,  2.8515e-01,  2.8846e-01,  2.8857e-01,\n",
      "         2.9487e-01,  2.9503e-01,  2.9533e-01,  3.0428e-01,  3.0609e-01,\n",
      "         3.0847e-01,  3.0847e-01,  3.0930e-01,  3.1166e-01,  3.1171e-01,\n",
      "         3.1463e-01,  3.1859e-01,  3.2012e-01,  3.2491e-01,  3.2532e-01,\n",
      "         3.2813e-01,  3.2828e-01,  3.2866e-01,  3.3059e-01,  3.3061e-01,\n",
      "         3.3301e-01,  3.3366e-01,  3.3686e-01,  3.3698e-01,  3.3718e-01,\n",
      "         3.3995e-01,  3.4019e-01,  3.4123e-01,  3.4490e-01,  3.4506e-01,\n",
      "         3.4571e-01,  3.5011e-01,  3.5035e-01,  3.5100e-01,  3.5136e-01,\n",
      "         3.5376e-01,  3.5425e-01,  3.5566e-01,  3.5725e-01,  3.5906e-01,\n",
      "         3.6169e-01,  3.6190e-01,  3.6328e-01,  3.6578e-01,  3.6845e-01,\n",
      "         3.7027e-01,  3.7196e-01,  3.7270e-01,  3.7501e-01,  3.7535e-01,\n",
      "         3.7600e-01,  3.7672e-01,  3.7740e-01,  3.7769e-01,  3.8045e-01,\n",
      "         3.8089e-01,  3.8175e-01,  3.8314e-01,  3.8476e-01,  3.8478e-01,\n",
      "         3.8672e-01,  3.8718e-01,  3.9166e-01,  3.9546e-01,  3.9620e-01,\n",
      "         3.9666e-01,  4.0614e-01,  4.0690e-01,  4.0771e-01,  4.0895e-01,\n",
      "         4.1399e-01,  4.1436e-01,  4.2015e-01,  4.2117e-01,  4.2216e-01,\n",
      "         4.2405e-01,  4.2491e-01,  4.2977e-01,  4.3114e-01,  4.3337e-01,\n",
      "         4.4713e-01,  4.4770e-01,  4.4891e-01,  4.4997e-01,  4.5542e-01,\n",
      "         4.5842e-01,  4.6579e-01,  4.6659e-01,  4.6820e-01,  4.6882e-01,\n",
      "         4.7232e-01,  4.7251e-01,  4.7329e-01,  4.7383e-01,  4.7525e-01,\n",
      "         4.7595e-01,  4.7910e-01,  4.7921e-01,  4.7929e-01,  4.8174e-01,\n",
      "         4.8400e-01,  4.8753e-01,  4.8881e-01,  4.8931e-01,  4.8939e-01,\n",
      "         4.9019e-01,  4.9071e-01,  4.9146e-01,  4.9241e-01,  4.9656e-01,\n",
      "         4.9888e-01,  4.9908e-01,  5.0237e-01,  5.0349e-01,  5.0554e-01,\n",
      "         5.0863e-01,  5.1016e-01,  5.1391e-01,  5.1513e-01,  5.1973e-01,\n",
      "         5.2105e-01,  5.2130e-01,  5.2242e-01,  5.2400e-01,  5.2800e-01,\n",
      "         5.4119e-01,  5.4204e-01,  5.4254e-01,  5.4388e-01,  5.4552e-01,\n",
      "         5.4784e-01,  5.4982e-01,  5.5070e-01,  5.5497e-01,  5.5580e-01,\n",
      "         5.5690e-01,  5.5705e-01,  5.6040e-01,  5.6078e-01,  5.7211e-01,\n",
      "         5.7229e-01,  5.7545e-01,  5.7794e-01,  5.7874e-01,  5.8110e-01,\n",
      "         5.8338e-01,  5.8389e-01,  5.8389e-01,  5.8633e-01,  5.8671e-01,\n",
      "         5.8901e-01,  5.8971e-01,  5.9438e-01,  5.9501e-01,  5.9935e-01,\n",
      "         6.0010e-01,  6.0783e-01,  6.0834e-01,  6.1031e-01,  6.1034e-01,\n",
      "         6.1069e-01,  6.1127e-01,  6.1579e-01,  6.1733e-01,  6.1785e-01,\n",
      "         6.1902e-01,  6.2598e-01,  6.2667e-01,  6.2995e-01,  6.3120e-01,\n",
      "         6.3128e-01,  6.3292e-01,  6.3504e-01,  6.3860e-01,  6.4079e-01,\n",
      "         6.4215e-01,  6.4784e-01,  6.4802e-01,  6.4811e-01,  6.4932e-01,\n",
      "         6.5001e-01,  6.5114e-01,  6.5151e-01,  6.5266e-01,  6.6477e-01,\n",
      "         6.6646e-01,  6.6851e-01,  6.6905e-01,  6.7056e-01,  6.7066e-01,\n",
      "         6.7288e-01,  6.7518e-01,  6.7682e-01,  6.7878e-01,  6.8374e-01,\n",
      "         6.8427e-01,  6.8585e-01,  6.8662e-01,  6.8717e-01,  6.9553e-01,\n",
      "         6.9868e-01,  6.9876e-01,  6.9903e-01,  7.0059e-01,  7.0342e-01,\n",
      "         7.0775e-01,  7.1714e-01,  7.1880e-01,  7.2040e-01,  7.2401e-01,\n",
      "         7.2500e-01,  7.2658e-01,  7.3581e-01,  7.4552e-01,  7.4645e-01,\n",
      "         7.4916e-01,  7.4950e-01,  7.6345e-01,  7.6543e-01,  7.6741e-01,\n",
      "         7.7138e-01,  7.7156e-01,  7.7232e-01,  7.7239e-01,  7.7384e-01,\n",
      "         7.7801e-01,  7.9681e-01,  8.0269e-01,  8.0455e-01,  8.1310e-01,\n",
      "         8.2337e-01,  8.2872e-01,  8.3091e-01,  8.3107e-01,  8.3356e-01,\n",
      "         8.3424e-01,  8.3495e-01,  8.3787e-01,  8.3949e-01,  8.4382e-01,\n",
      "         8.4423e-01,  8.4672e-01,  8.5039e-01,  8.5112e-01,  8.7523e-01,\n",
      "         8.7630e-01,  8.7958e-01,  8.8296e-01,  8.8666e-01,  8.8919e-01,\n",
      "         8.9664e-01,  8.9932e-01,  9.0074e-01,  9.0159e-01,  9.0866e-01,\n",
      "         9.0943e-01,  9.1123e-01,  9.1992e-01,  9.2744e-01,  9.3827e-01,\n",
      "         9.4657e-01,  9.5037e-01,  9.5487e-01,  9.5793e-01,  9.5798e-01,\n",
      "         9.6383e-01,  9.7344e-01,  9.7385e-01,  9.8409e-01,  9.8536e-01,\n",
      "         1.0176e+00,  1.0293e+00,  1.0843e+00,  1.1097e+00,  1.1151e+00,\n",
      "         1.1214e+00,  1.1303e+00,  1.1378e+00,  1.1424e+00,  1.1873e+00,\n",
      "         1.1905e+00,  1.2026e+00,  1.2122e+00,  1.2283e+00,  1.2398e+00,\n",
      "         1.2447e+00,  1.2490e+00,  1.3433e+00,  1.3456e+00,  1.3460e+00,\n",
      "         1.3469e+00,  1.3578e+00,  1.3614e+00,  1.3737e+00,  1.3895e+00,\n",
      "         1.4476e+00,  1.4969e+00,  1.5747e+00,  1.6372e+00,  1.8727e+00],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(1, 3, 224, 224).cuda()\n",
    "print(torch.unique(model(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcad23ae-49fa-492e-9339-eec203754a50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
